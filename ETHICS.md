# ğŸ›¡ï¸ ETHICS.md
Este documento descreve os princÃ­pios Ã©ticos, limitaÃ§Ãµes e boas prÃ¡ticas associadas ao uso do **Community Policy Classifier**, um projeto educacional de NLP para classificaÃ§Ã£o de frases **Adequadas vs. Potencialmente Violadoras de PolÃ­ticas de Comunidade**.

---

## ğŸ“Œ Finalidade do Projeto
- Projeto criado **exclusivamente para fins acadÃªmicos e de portfÃ³lio**.  
- NÃ£o deve ser usado em **produÃ§Ã£o** ou em ambientes reais de moderaÃ§Ã£o sem supervisÃ£o humana.  
- O dataset utilizado Ã© **fictÃ­cio e seguro**, evitando exposiÃ§Ã£o de exemplos reais de discurso de Ã³dio.

---

## âš ï¸ Riscos Identificados
1. **Falsos positivos**  
   - Frases adequadas podem ser incorretamente classificadas como violadoras.  
   - Risco: censura indevida, impacto na liberdade de expressÃ£o.

2. **Falsos negativos**  
   - Frases ofensivas podem nÃ£o ser detectadas.  
   - Risco: exposiÃ§Ã£o de usuÃ¡rios a conteÃºdo prejudicial.

3. **ViÃ©s algorÃ­tmico**  
   - Modelos de linguagem podem reproduzir ou amplificar vieses presentes no dataset.  
   - Risco: discriminaÃ§Ã£o injusta contra grupos especÃ­ficos.

4. **GeneralizaÃ§Ã£o limitada**  
   - O dataset Ã© pequeno e sintÃ©tico, nÃ£o reflete toda a diversidade linguÃ­stica real.  
   - Risco: mÃ¡ performance em contextos reais.

---

## âœ… Medidas de MitigaÃ§Ã£o
- **Uso restrito**: deixar claro no README e no prÃ³prio repositÃ³rio que o modelo Ã© didÃ¡tico.  
- **Explicabilidade**: aplicaÃ§Ã£o de **LIME e SHAP** para entender as decisÃµes do modelo.  
- **Dataset fictÃ­cio**: nÃ£o contÃ©m frases reais sensÃ­veis, reduzindo risco de vazamento ou exposiÃ§Ã£o indevida.  
- **SupervisÃ£o humana**: reforÃ§ar que modelos desse tipo devem ser usados apenas como suporte, nunca como decisÃ£o final.

---

## ğŸ” Boas PrÃ¡ticas Recomendadas
- **Treinamento com dados reais** â†’ apenas em contextos controlados, anonimizados e com consentimento.  
- **ValidaÃ§Ã£o por especialistas** â†’ linguistas, juristas e equipes de Trust & Safety devem revisar modelos de moderaÃ§Ã£o em produÃ§Ã£o.  
- **AtualizaÃ§Ã£o contÃ­nua** â†’ linguagem ofensiva muda ao longo do tempo, exigindo re-treinamento frequente.  
- **Monitoramento de mÃ©tricas Ã©ticas** â†’ como *false positive rate* e *false negative rate* em diferentes grupos de usuÃ¡rios.

---

## ğŸš« LimitaÃ§Ãµes do Projeto
- NÃ£o Ã© uma ferramenta oficial de moderaÃ§Ã£o.  
- NÃ£o substitui revisÃ£o humana.  
- NÃ£o cobre mÃºltiplos idiomas ou contextos culturais.  
- Resultados apresentados servem apenas para **fins ilustrativos e educacionais**.

---

## ğŸ“– ReferÃªncias
- Davidson et al. (2017) â€“ *Automated Hate Speech Detection and the Problem of Offensive Language*  
- Jigsaw (Google) â€“ *Toxic Comment Classification Challenge*  
- IEEE Ethically Aligned Design Guidelines  

---

âœï¸ **Autor:** Josiele Ferreira  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/josieleferreira/)  
